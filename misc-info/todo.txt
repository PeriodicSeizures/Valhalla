# todo:

- im tired of this prob gonna make a new project
    too many bugs
    
    too many ideas that cannot be implemented because of this confined design

- socket has a failed assignment or something in peer
    

- bring back 'inline' zdo encoding
    idk what else to call it
    
    takes advantage of how Valheim generates UIDs
        uid is signed, and utilizes int range * 2 (unity random int unioned with ip C# hash)
        
        ~31 bits are unused, pleeenty of capacity for id (or until devs change gen algo)

- zdos created by player are being duplicated
    same ids, zero revs, (zero prefab?)
    
    so far only within zoneZDOs
    
    zdo must be getting corrupt somewhere

- owners not working too
    only sometimes

- const qualifying zdoid seemingly fixed this
    nvm
    
    seemingly zdos near spawn have this issue
    
    in CreateSyncList sort, 2 different zdos, with same id, but different prefab (one is 0..)
        why are ids same across different objects
        why is prefab = 0?
        
        almost seems like this zdo failed initialization
        
        why does this exist
        
        likely zdo hash / compare not working?
            set not containing unique zdos,
            
            or ...
            
            ubf?
            
            prevent hash to be 0

- zdo zones broken again
    zdo zone not matching expected

- discord player spawn mistaken for death

- benchmark / test:
    - map<zdoid, map<u64, variant>>
    
        pros: 
            access all zdo members with single retrieval
        cons:
            long hashes are huge
            all members
            
        HEAP: +1,300,000.00 KB
    
    - tuple<map<zdoid, map<hash, T>>...>
        pre Load() -- prior to first zdo
        HEAP: +1,098,559.70 KB
        post ZDOManager load()

- add Find to ZDO : to get data_map from globals
    Find<T>(GetID())
    
    much needed

- perfect encoded zdoid seems to be 4/4
    little compromises
        - half capacity of current
        - no padding
        - most contiguous members are aligned / no packing
        - alignment with member map
        - perfect hash
            - does ankerl care about hash uniqueness...

- max opt zdo:
    26B * 2M = 52'000'000 = 52MB
    
    split member maps
    
    zdoid(16) with type:
        16+4 = 20
        16+12 = 28
        16+16 = 32
        16+4 = 20
        16+8 = 24
        
        no padding issues
        
    zdoid(8) with type:
        8+4 = 12
        8+12 = 20
        8+16 = 32
        8+4 = 20
        8+8 = 24
        
        no padding issues
        
        negligible since
        
    no padding issues regarding 8-bit zdoid
    
    4-byte padding applied to 4-byte zdoid
        waste and limited capacity

- is shrinkage worth it
    zdoid to 4 bytes, save 4 bytes:
    
    what is comparison here:
        
        assuming 2 million zdos in world
        
        padding (8+4+4):
            16 bytes * 2M = 32'000'000 = 32MB
        
        packed (8+4):
            12 bytes * 2M = 24'000'000 = 24MB
            
        encoded (4+4):
            8 bytes * 2M = 16'000'000 = 16MB
            pool: maybe 8 * 2^16 B = 8 * 65536 = 524'288 = 0.5MB (still a lot; conservative)
            
        highly-encoded (2+2):
            4 bytes * 2M = 8'000'000 = 8MB ()
            
            alloc can be changed
                16 bits is too much; even on highly active / changing env; 
                userID can be larger
            
            
        
        
        

- sizes
    id: 4 (8/24: user/id)
    pos: 12 (8/8/8: x/y/z)
    rev: 4
    rot: 4 (10/12/10: rx, ry, rz)
    prefab: 2
    
    prior to finalizing an approach, 
        4-byte zdoid, up to 4 billion zdos
        zdoid userid essentially ignores userIDs, just alloc zdoids with increasing 
            userIDs if zdoid id bits for a given userID become exhausted
        
        2^20 better for id (high-end)
            2^18 low end
            
        depends on owner bits exhaustion rate
        
        assuming 1 new owner per 1 seconds (extremely high rate)
        
        12 userID bits = 4096/(60s) = 1.13 min
        
        2^16 = 65536/(60s) = 18 hours much better
        
        // 16 bits each
        
        

- singular member map might also be too large
    map<id, map<ehash, variant>>

    variant takes up a lot for all these types,
    additional iteration is unnecessary,
    might be better to have multiple member maps like in valheim

- zdo map taking a long time
    compare seems expensive,
    also unique ptr construct every instantiate
    
    i believe the problem is using many indirections, which is odd because ptr->... 
    is not that expensive, but i guess it does when done thousands per second

    - contiguius map is not the issue
        - only 16MB with a "huge" map

    - TRY THIS: increase size is expensive (use segmented vector)
    
    - ptr get expensive?
    - 

- simplify filter
    - simple set, not a composite filter

- lua usage for zdo collector
    -- either collect by:
        all, zone, prefab, owned

        how to collect
    
    iterator() -- will select a iterator of zdos 
    
    iterator()

    local collect = ZDOCollector.new(CollectMode.zone)
        
        :range(player.zdo.position, 32, RangeMode.zone) -- whether to include zones, manhattan dist, normal
        :filter(function(zdo) return zdo:GetString('example') ~= '' end)
        :transform(function(zdo) return zdo:GetID() end) -- no operation should pass unmanaged ptr to lua
        :foreach(function(id) return zdo:GetID() end)
        --:tolist to_vector ()

- rather than giving lua access to ranges/views 
    during GetZDOs / SomeZDOs ...
    accept a filter along with foreach function
    
    during zdo modify functions, pass zdo by value,
        when zdo is to be modified,
        make lua return it?
            this way pointers are never used
            and access is still fast?
            

- the only place zdos become unstable are
    insertion / deletion

    this is a problem when zdo& are stored during a modification (insert/delete)

    where is this?
        _Instantiate / Instantiate
        _EraseZDO / DestroyZDO

    anywhere mods are given access to modify zdo container, mitigate this
        instead make changes take effect after-the-fact

- memory cost?
    - set<zdo>: zdo (40) =                              40
        - pros: memory efficient
        - cons: requires modified set / not really portable, unstable pointer
            WAIT just const_cast away the constness?
    - set<zdo*>: ptr (8) + zdo (40) =                   48
        - pros: memory efficient, stable pointer
        - cons: requires modified set / not really portable
    - map<id, data>: id (8) + data (32) =               40 
        - pros: no id anomalies, memory efficient
        - cons: requires wrapper
    - map<id, zdo>: id (8) + data (40) =                48
        - pros: id stored with zdo
        - cons: id anomalies; more memory consumption
    - map<id, data*>: id (8) + ptr (8) + data (32) =    48 (requires wrapper; no id anomalies, memory efficient)
    - map<id, zdo*>: id (8) + ptr (8) + data (40) =     56 (id anomalies; more memory consumption)

- move back to map<id, zdo>
    - 

- to keep zdos stable with mods
    - prevent insertion in-between lua ops
    - prevent mods from directly inserting/deleting

- can use ranges/transformers for zone

- alternative for lua zdos (semi-handles, nothing shared)
    have an overwrite setting for zdo-set
    manually sets a zdo of id

- const ref instantiations can remain, 
    anything involving ownership sharing must use <local> shared ptr

- dpp is somehow forcing c++17
    temporary fix: just disable discord compat for now

    https://github.com/brainboxdotcc/DPP/issues/658

    dpp is still leaking compiler flags, overriding user flags

- is openssl/zlib/tracy/zstd really needed?
    extra libraries that arent upmost required

    valheim does backup worlds, but not compressed

    renaming the old world as a backup is faster than reading and compressing it
    maybe not efficient for disk space, but no compression is required

    maybe make this a compile option? yes

- prioritize full lua safety
    make Peer use shared ptr
        i hate this but its necessary
    
    make ZDO when accessed by lua, a value-type
        no more ptrs
        
        copy zdo over to lua
            a zdo alone is ~40 bytes, rather smaller 
            than it used to be
    

- refactor everything to follow a format:
    class members: m_snake_case
    class names: PascalCase
    methods: snake_case()
    global constants: g_snake_case
    enums: PascalCase { ValueInPascalCase }
    
    
    lua api:
        class names: PascalCase
        class member/method names: snake_case

- preliminary tests showed routemanager might not be working correctly
    lagg?
    
    is forwarding correctly implemented?
    
    
- check on destroy zdo,

    it might not work either
    
    


- datarev use
    adjust pack sizes for rev

- sync togethor admin set + peer flags
    for some reason Valhalla()->admin showed peer steamid, 
    but not when accessed from NetManager

- use zdo as key/hash in set<>
    - use less memory
        not only avoidance of dual id,
        but zdo can now directly contain ID
        
        and not be separately contained in a wrapper class

- by using the zdo itself as the key/hash

- alternative for storing zdos in map
    use set not map
    
    zdo will contain id again
    
    id will be the key

- if vector axis' are different from each other, then scale, else scale_scalar
    determining equality without comparing
    
- consider removing owner bits from pack
    most zdos will remain unowned
    
- batched owning
    most zdos within a zone will be owned by a specific peer, 
    
    exceptions are made for individual RequestOwn() znetview rpc methods, like 
        container open, 
        any case where ownership is transferred
        
        determining batched ownership is simple, assume all zdoss in region are owned by specific predefined peer, queried by ZoneID
        
    every time a peer disconnects, all zdos are iterated
        why
        
        would be better to iterate all owned zdos from ZDO_OWNERS
        
        

- severe latency
    debug mode?
    
    zdos also do not appear to get destroyed on other clients

- how to disable tracy profiler
    it takes up memory constantly until out of memory

- implement new random event system
    RPC_ConsoleStartRandomEvent
    
    ResetRandomEvent too
    
    player event data
    
- rotations during world generation is screwed up
    trees are upside down

- change login defaults
    - disable password
    - enable whitelist
    
    the password mechanic is dumb, at least offline MC servers understand this; having password + 2fa auth like email at the least

- implement dungeon randomSpawns functionality

- dump starting keys
    remove uneeded members
    use bitpack / masks for bool-like

- dump dungeon randomSpawns

- optimize dumped .pkg

- free unused heightmaps
    or could generate everything

- fix server shutdown routines (might be unsafe if exits are premature)
    - save world, 
    - proper cleanup / disposal of managers

- global naming refactor
    simplify names
    
    remove redundant names
        remove prefixes that are the same as their namespace
        
        ZDO::ZDO_member_map
              ^              
    move to a more standard naming convention
        either all underscores, or all pascalcase...
        
    make sure bitpack has full asserts and compiletime restricts
        it likes to complain

    store all usings from IZDOManager in ZDO

- planned optimizations to zdo:
    - ZDOID removals / full consilidation within ZDOManager
        zdos will no longer contain their own zdoid, somewhat better because
        the id in ZDOManager will no longer have a possibility of mismatching
        with its zdo
        
        as seen in 2 different branches which attempted to do exactly this, 
        the complications of removing ID() from zdo are insane
        
        the proposal for implementing this are as follows:
        
        a ZDO Data structure (ZDO_raw)
            holds data of zdo, little to no utility methods
        
        a ZDO wrapper (ZDO)
            holds the ZDO_raw& + id
            contains all methods originally within zdo
        
    - zdo.Owner VS zdo.zdoid.UserID
        these are essentially the same type, however their domain differs
        - zdo.Owner is limited to current peers
        - zdo.zdoid.UserID is limited to all peer sessions ever generated
            
    - store ZDO owner externally
        because not all zdos are owned (only a very select few)

- consider removing denotions and all hints until stable
    member hints are not stable (easy to forget to remove or add) 
    
    markers could be isolate numerical (separate pack indices instead of entire localdenotions)...

- reset member flag(s) when member extracted from ZDO
    also reset connectors
    
- might be able to shrink zdo connector value_type pair    
    type is a few bits (2 values are significant), 
    prefab target can be an index instead of 32-bit
